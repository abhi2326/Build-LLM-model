# Build-LLM-model-using-ollama-and-Docker
# Open WebUI ðŸ‘‹  

**Open WebUI** is a powerful, extensible, and offline-capable self-hosted platform for working with Large Language Models (LLMs). It seamlessly integrates with **Ollama**, **OpenAI-compatible APIs**, and provides built-in features for **RAG (Retrieval Augmented Generation)**, **Python function calling**, **plugin support**, and more.


## ðŸŒŸ Key Features

- ðŸš€ **Quick Setup**: Deploy effortlessly using Python `pip`, Docker, or Kubernetes (via `kubectl`, Helm, or Kustomize).
- ðŸ¤– **Model Flexibility**: Supports Ollama and OpenAI APIs, and connects with services like GroqCloud, LMStudio, Mistral, OpenRouter, and more.
- ðŸ” **Role-Based Access Control (RBAC)**: Create user roles with custom permissions to maintain security and user-specific access.
- ðŸ“± **Responsive and PWA**: Optimized for desktop and mobile; install as a Progressive Web App for offline access.
- ðŸ§  **Local RAG Support**: Load documents into your chat environment for context-aware conversation via RAG.
- âœï¸ **Markdown & LaTeX**: Full support for Markdown formatting and LaTeX math rendering.
- ðŸŽ¤ **Voice/Video Chat**: Built-in support for hands-free communication via voice and video.
- ðŸŽ¨ **Image Generation**: Integrate with tools like ComfyUI, AUTOMATIC1111, and DALLÂ·E for image generation.
- ðŸ **Native Python Integration**: Write and execute custom Python functions directly in the chat.
- ðŸŒ **Web Browsing & Search**: Inject real-time web search results from SearXNG, Brave, Bing, and more into conversations.
- ðŸŒ **Multilingual UI**: Internationalized interface with community-driven language support.
- ðŸ§© **Plugin System**: Extend functionality.
- ðŸ“ˆ **Monitoring & Rate Limiting**: Track usage, apply limits, and integrate with tools like Langfuse.

Explore more features in the [official documentation](https://docs.openwebui.com/features).

---

## ðŸ”§ Installation Options

### ðŸ”¹ Option 1: Install via pip (Python 3.11)

```bash
pip install open-webui
open-webui serve
```

**Access at: [http://localhost:8080](http://localhost:8080)**

---

### ðŸ”¹ Option 2: Install via Docker

> ðŸ“Œ Ensure you mount the database volume using `-v open-webui:/app/backend/data` to prevent data loss.

**Basic Docker Command (with Ollama installed locally):**
```bash
docker run -d -p 3000:8080 \
--add-host=host.docker.internal:host-gateway \
-v open-webui:/app/backend/data \
--name open-webui --restart always \
ghcr.io/open-webui/open-webui:main
```

**Remote Ollama Server:**
```bash
docker run -d -p 3000:8080 \
-e OLLAMA_BASE_URL=https://your-ollama-server.com \
-v open-webui:/app/backend/data \
--name open-webui --restart always \
ghcr.io/open-webui/open-webui:main
```

**With NVIDIA GPU Support (CUDA):**
```bash
docker run -d -p 3000:8080 \
--gpus all \
--add-host=host.docker.internal:host-gateway \
-v open-webui:/app/backend/data \
--name open-webui --restart always \
ghcr.io/open-webui/open-webui:cuda
```

**Using Only OpenAI API:**
```bash
docker run -d -p 3000:8080 \
-e OPENAI_API_KEY=your_secret_key \
-v open-webui:/app/backend/data \
--name open-webui --restart always \
ghcr.io/open-webui/open-webui:main
```

